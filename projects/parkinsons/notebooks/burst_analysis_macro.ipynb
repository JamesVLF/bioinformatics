{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8817b395",
   "metadata": {},
   "source": [
    "#### LEVEL 2: Identify broad-scale, physiologically-relevant changes in burst features and timing between control and drugged samples\n",
    "    (1) Burst-Aligned IFR Matrices and Averaging\n",
    "        - What does activity look like centered on burst peaks?\n",
    "        - Are units consistently recruited in a pattern across bursts?\n",
    "        - Can we compute an average firing pattern across multiple bursts?\n",
    "            - Extract IFR matrices centered on each burst\n",
    "            - Sort units by peak firing time in each burst\n",
    "            - Average across bursts for a clean picture\n",
    "\n",
    "#### LEVEL 3: Single Burst Raster and Unit Order\n",
    "    - Which units fired during a single burst?\n",
    "    - When did they peak?\n",
    "    - What is their temporal order?\n",
    "    - Can I visualize a clear activation sequence?\n",
    "        - IFR matrix for a burst and sort units by peak firing rate within that window.\n",
    "\n",
    "#### LEVEL 4: Relative Timing and Firing Order\n",
    "    Goals:\n",
    "        Analyze when each unit fires relative to the population burst peak across bursts\n",
    "        Add temporal precision to burst structure metrics for rank order correlations and functional role identification.\n",
    "            - How early or late does a unit fire?\n",
    "            - Do some units always fire early? Are they consistent?\n",
    "            - Can we detect sequential recruitment or backbone structures?\n",
    "        Needs:\n",
    "            A list of time differences between:\n",
    "                Each unit's peak activity\n",
    "                Population peak in the same burst\n",
    "        Tasks:\n",
    "            - Build histograms of activation order\n",
    "            - Identify early-firing units\n",
    "            - Compare temporal dispersion of units across bursts\n",
    "        Ideas for expansion:\n",
    "            - Group units by quartiles of firing latency\n",
    "            - Track variability in firing latency across bursts\n",
    "            - Identify scaffold units by consistently early peaks\n",
    "            - Identify functional units by consistent temporal correlations across bursts\n",
    "\n",
    "#### LEVEL 5: Pairwise and Burst-to-Burst Relationships\n",
    "    - How similar are units to each other during bursts?\n",
    "    - Do certain pairs co-activate reliably?\n",
    "    - Do bursts follow similar patterns over time?\n",
    "    - Can I detect structure across the network beyond global averages?\n",
    "        Tasks:\n",
    "            - Compute pairwise correlation (Pearson), burst similarity (cosine), and possibly latency matrices.\n",
    "            - Incorporate dimensional reduction (PCA/t-SNE) and cluster analysis with rich burst metrics acquired above.\n",
    "### Navigation tips for S3 and Kubernetes\n",
    "\n",
    "Quickly list (`ls`) or copy (`cp`) .acqm.zip files to cwd:\n",
    "```\n",
    "for UUID in \\\n",
    "  2025-06-09-e-MaxTwo_D57_KOLF2.2J_SmitsMidbrain_6OHDA_T2_72hr/derived/kilosort2 \\\n",
    "  2025-06-10-e-MaxTwo_D58_KOLF2.2J_SmitsMidbrain_6OHDA_T2_D4/derived/kilosort2 \\\n",
    "  2025-06-11-e-MaxTwo_D59_KOLF2.2J_SmitsMidbrain_6OHDA_T2_D5/derived/kilosort2 \\\n",
    "  2025-06-12-e-MaxTwo_D60_KOLF2.2J_6OHDA_T2_D6/derived/kilosort2\n",
    "do\n",
    "  echo \"Scanning $UUID...\"\n",
    "  aws --endpoint-url https://s3.braingeneers.gi.ucsc.edu s3api list-objects-v2 \\\n",
    "    --bucket braingeneers \\\n",
    "    --prefix ephys/${UUID}/ \\\n",
    "    --query \"Contents[?ends_with(Key, '_acqm.zip')].[Key]\" \\\n",
    "    --output text |\n",
    "  while read KEY; do\n",
    "    if [ -n \"$KEY\" ] && [ \"$KEY\" != \"None\" ]; then\n",
    "      FILE=$(basename \"$KEY\")\n",
    "      echo \"Downloading $FILE...\"\n",
    "      aws --endpoint-url https://s3.braingeneers.gi.ucsc.edu s3 ls \\\n",
    "        s3://braingeneers/\"$KEY\" \"./$FILE\"\n",
    "    fi\n",
    "  done\n",
    "done\n",
    "```\n",
    "FILE_TYPE=\".h5\"\n",
    "\n",
    "for UUID in \\\n",
    "  2025-06-09-e-MaxTwo_D57_KOLF2.2J_SmitsMidbrain_6OHDA_T2_72hr/derived/kilosort2 \\\n",
    "  2025-06-10-e-MaxTwo_D58_KOLF2.2J_SmitsMidbrain_6OHDA_T2_D4/derived/kilosort2 \\\n",
    "  2025-06-11-e-MaxTwo_D59_KOLF2.2J_SmitsMidbrain_6OHDA_T2_D5/derived/kilosort2 \\\n",
    "  2025-06-12-e-MaxTwo_D60_KOLF2.2J_6OHDA_T2_D6/derived/kilosort2\n",
    "do\n",
    "  echo \"Scanning $UUID for '*$FILE_TYPE'...\"\n",
    "\n",
    "  aws --endpoint-url https://s3.braingeneers.gi.ucsc.edu s3api list-objects-v2 \\\n",
    "    --bucket braingeneers \\\n",
    "    --prefix ephys/${UUID}/ \\\n",
    "    --query \"Contents[?ends_with(Key, \\`${FILE_TYPE}\\`)].Key\" \\\n",
    "    --output text |\n",
    "  while read -r KEY; do\n",
    "    if [ -n \"$KEY\" ]; then\n",
    "      echo \"Found: $KEY\"\n",
    "    fi\n",
    "  done\n",
    "done\n",
    "\n",
    "FILE_TYPE=\"_.acqm.zip\"\n",
    "\n",
    "for UUID in \\\n",
    "  2025-06-09-e-MaxTwo_D57_KOLF2.2J_SmitsMidbrain_6OHDA_T2_72hr/derived/kilosort2 \\\n",
    "  2025-06-10-e-MaxTwo_D58_KOLF2.2J_SmitsMidbrain_6OHDA_T2_D4/derived/kilosort2 \\\n",
    "  2025-06-11-e-MaxTwo_D59_KOLF2.2J_SmitsMidbrain_6OHDA_T2_D5/derived/kilosort2 \\\n",
    "  2025-06-12-e-MaxTwo_D60_KOLF2.2J_6OHDA_T2_D6/derived/kilosort2\n",
    "do\n",
    "  echo \"Scanning $UUID for files containing '$FILE_TYPE'...\"\n",
    "\n",
    "  aws --endpoint-url https://s3.braingeneers.gi.ucsc.edu s3api list-objects-v2 \\\n",
    "    --bucket braingeneers \\\n",
    "    --prefix ephys/${UUID}/ \\\n",
    "    --query 'Contents[].Key' \\\n",
    "    --output text |\n",
    "  grep \"$FILE_TYPE\" |\n",
    "  while read -r KEY; do\n",
    "    if [ -n \"$KEY\" ]; then\n",
    "      echo \"Found: $KEY\"\n",
    "    fi\n",
    "  done\n",
    "done\n",
    "\n",
    "\n",
    "\n",
    "Search by keyword in UUID:\n",
    "```\n",
    "aws --endpoint-url https://s3.braingeneers.gi.ucsc.edu s3 ls s3://braingeneers/ephys/ --recursive | grep e-MaxTwo_D57\n",
    "```\n",
    "Search for one UUID:\n",
    "```\n",
    "aws --endpoint-url https://s3.braingeneers.gi.ucsc.edu s3 ls s3://braingeneers/ephys/2025-06-09-e-MaxTwo_D57_KOLF2.2J_SmitsMidbrain_6OHDA_T2_72hr/ --recursive\n",
    "```\n",
    "List all files in a bucket:\n",
    "```\n",
    "aws --endpoint-url https://s3.braingeneers.gi.ucsc.edu s3api list-objects-v2 \\\n",
    "  --bucket braingeneers \\\n",
    "  --prefix ephys/ \\\n",
    "  --delimiter / \\\n",
    "  --query \"CommonPrefixes[].Prefix\" \\\n",
    "  --output text\n",
    "```\n",
    "View credentials:\n",
    "`\n",
    "grep 'aws_access_key_id' ~/.aws/credentials\n",
    "grep 'aws_secret_access_key' ~/.aws/credentials\n",
    "`\n",
    "In Kubernetes, create a secret in my cluster named s3 credentials:\n",
    "`\n",
    "kubectl create secret generic s3-credentials \\\n",
    "  --from-literal=AWS_ACCESS_KEY_ID=\"ID123\" \\\n",
    "  --from-literal=AWS_SECRET_ACCESS_KEY=\"KEY123\"\n",
    " `\n",
    "Confirm secret was created: `kubectl get secrets`\n",
    "View what's inside: `kubectl get secret s3-credentials -o yaml`\n",
    "Decode and read later: `kubectl get secret s3-credentials -o jsonpath=\"{.data.AWS_ACCESS_KEY_ID}\" | base64 --decode`\n",
    "\n",
    "Help commands in s3:\n",
    "    High-level commands (ls, cp, sync): `aws s3 ls help`\n",
    "    Low-level api commands(`list-objects-v2`, `get-object`): `aws s3api list-objects-v2 help`\n",
    "\n",
    "List all commands in Kubernetes: `kubectl`\n",
    "Info about a specific commmand: `kubectl <command> --help`\n",
    "List of command-line options for all commands: `kubectl options`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set up imports and paths ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the project root so Python can find analysis_libs and others\n",
    "project_root = Path(\"~/bioinformatics\").expanduser().resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Define the data directory\n",
    "data_path = project_root / \"data/extracted/maxtwo_run2\"\n",
    "\n",
    "# Import burst analysis tools\n",
    "from burst_analysis.loading import SpikeDataLoader\n",
    "from burst_analysis.detection import BurstDetection\n",
    "from burst_analysis.plotting import plot_overlay_raster_population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Load one dataset\n",
    "# -----------------------------\n",
    "data_path = Path(\"~/bioinformatics/data/extracted/maxone_run1\").expanduser()\n",
    "loader = SpikeDataLoader(data_path)\n",
    "datasets = loader.load()\n",
    "\n",
    "# Pick one dataset\n",
    "key = list(datasets.keys())[0]\n",
    "spike_data = datasets[key]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Configure and run detector\n",
    "# -----------------------------\n",
    "config = {\n",
    "    \"bin_size_ms\": 4,\n",
    "    \"square_win_ms\": 5,\n",
    "    \"gauss_win_ms\": 30,\n",
    "    \"threshold_rms\": 2.0,\n",
    "    \"min_dist_ms\": 200,\n",
    "    \"min_merge_separation_ms\": 2000,\n",
    "    \"burst_edge_fraction\": 0.25\n",
    "}\n",
    "\n",
    "bd = BurstDetection(spike_data.train, fs=spike_data.metadata.get(\"fs\", 10000), config=config)\n",
    "\n",
    "result = bd.compute_population_rate_and_bursts()\n",
    "times, smoothed, peaks, peak_times, bursts, burst_windows = result\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Plot raster + population\n",
    "# -----------------------------\n",
    "plot_overlay_raster_population(\n",
    "    trains=spike_data.train,\n",
    "    times=times,\n",
    "    smoothed=smoothed,\n",
    "    bursts=bursts,\n",
    "    dataset_label=key,\n",
    "    time_range=(0, 60)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
